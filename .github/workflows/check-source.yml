name: Check source article for updates

on:
  schedule:
    - cron: '0 14 * * *' # Daily at 7am PT / 2pm UTC
  workflow_dispatch:

permissions:
  contents: write
  issues: write

jobs:
  check-source:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Fetch and extract article content
        id: fetch
        run: |
          ARTICLE_URL="https://www.independent.com/2026/02/05/burger-week-2026/"

          # Fetch the page
          curl -s -L -o page.html "$ARTICLE_URL"

          # Extract only the article body content (restaurant list + details)
          # Uses Python's built-in html.parser — no pip dependencies needed
          python3 << 'PYEOF'
          import html.parser
          import re

          class ArticleExtractor(html.parser.HTMLParser):
              def __init__(self):
                  super().__init__()
                  self.in_target = False
                  self.depth = 0
                  self.text_parts = []
                  self.skip_tags = {"script", "style", "noscript"}
                  self.skip_depth = 0

              def handle_starttag(self, tag, attrs):
                  attrs_dict = dict(attrs)
                  cls = attrs_dict.get("class", "")
                  # Match the article content container
                  if "wkwp-post-content" in cls:
                      self.in_target = True
                      self.depth = 1
                      return
                  if self.in_target:
                      self.depth += 1
                      if tag in self.skip_tags:
                          self.skip_depth += 1

              def handle_endtag(self, tag):
                  if self.in_target:
                      if tag in self.skip_tags and self.skip_depth > 0:
                          self.skip_depth -= 1
                      self.depth -= 1
                      if self.depth <= 0:
                          self.in_target = False

              def handle_data(self, data):
                  if self.in_target and self.skip_depth == 0:
                      text = data.strip()
                      if text:
                          self.text_parts.append(text)

          with open("page.html", "r", encoding="utf-8", errors="replace") as f:
              page = f.read()

          parser = ArticleExtractor()
          parser.feed(page)
          cleaned = "\n".join(parser.text_parts)

          # Normalize whitespace for consistent hashing
          cleaned = re.sub(r"[ \t]+", " ", cleaned)
          cleaned = re.sub(r"\n{3,}", "\n\n", cleaned)
          cleaned = cleaned.strip() + "\n"

          with open("cleaned.txt", "w") as f:
              f.write(cleaned)

          print(f"Extracted {len(parser.text_parts)} text segments from article body")
          PYEOF

          # Generate SHA-256 hash
          HASH=$(sha256sum cleaned.txt | awk '{print $1}')
          echo "hash=$HASH" >> "$GITHUB_OUTPUT"
          echo "Current content hash: $HASH"

      - name: Compare against stored hash
        id: compare
        run: |
          HASH="${{ steps.fetch.outputs.hash }}"
          HASH_FILE=".github/source-hash.txt"

          if [ ! -f "$HASH_FILE" ]; then
            echo "No previous hash found — first run"
            echo "changed=false" >> "$GITHUB_OUTPUT"
            echo "first_run=true" >> "$GITHUB_OUTPUT"
          elif [ "$(cat "$HASH_FILE")" = "$HASH" ]; then
            echo "Hash matches — no changes detected"
            echo "changed=false" >> "$GITHUB_OUTPUT"
            echo "first_run=false" >> "$GITHUB_OUTPUT"
          else
            echo "Hash differs — content has changed!"
            echo "changed=true" >> "$GITHUB_OUTPUT"
            echo "first_run=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Open GitHub Issue
        if: steps.compare.outputs.changed == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          TITLE="Source article updated — review data.js"
          DATE=$(date -u +"%Y-%m-%d %H:%M UTC")
          ARTICLE_URL="https://www.independent.com/2026/02/05/burger-week-2026/"

          # Check for existing open issue with same title
          EXISTING=$(gh issue list --state open --search "$TITLE" --json title -q '.[].title' | grep -F "$TITLE" || true)

          if [ -n "$EXISTING" ]; then
            echo "Open issue already exists — skipping creation"
          else
            printf 'The source article content has changed as of **%s**.\n\n' "$DATE" > /tmp/issue-body.md
            printf '**Article:** %s\n\n' "$ARTICLE_URL" >> /tmp/issue-body.md
            printf 'Please review the article for updates and sync any changes to `data.js`.\n\n' >> /tmp/issue-body.md
            printf -- '---\n' >> /tmp/issue-body.md
            printf '*Detected by the [check-source](%s) workflow.*\n' "$RUN_URL" >> /tmp/issue-body.md

            gh issue create --title "$TITLE" --body-file /tmp/issue-body.md
          fi

      - name: Commit updated hash
        if: steps.compare.outputs.changed == 'true' || steps.compare.outputs.first_run == 'true'
        run: |
          HASH="${{ steps.fetch.outputs.hash }}"
          echo "$HASH" > .github/source-hash.txt

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .github/source-hash.txt
          git commit -m "Update source article hash [skip ci]"
          git push

      - name: Fail if content changed
        if: steps.compare.outputs.changed == 'true'
        run: |
          echo "::error::Source article content has changed — review data.js"
          exit 1
